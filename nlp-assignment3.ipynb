{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Assignment 2\n### Zhengjie Deng a1865926","metadata":{}},{"cell_type":"markdown","source":"#### Dependency version:\n- Python: 3.8.16\n- pandas: 1.4.2\n- sklearn: 1.0.2\n- nltk: 3.7\n- tqdm: 4.64.1\n- matplotlib: 3.7.0\n- spacy: 3.5.0\n- numpy: 1.23.5\n- gensim: 3.8.3","metadata":{}},{"cell_type":"code","source":"import warnings\nimport gensim.downloader as api\nfrom collections import Counter\nimport spacy\nimport pandas as pd\nfrom tqdm import tqdm\nimport nltk\nfrom nltk.corpus import stopwords\n\n\nimport numpy as np\nimport json\nimport re\n\n\n# Install faiss\n!pip install faiss\n!pip install pyserini\n\n# Download stopwords\nnltk.download('stopwords')\nnltk.download('wordnet2022')\n\n# Download en_core_web_lg model\nspacy.cli.download(\"en_core_web_sm\")\n\n\n# Download the pre-trained GloVe model\nglove_model = api.load('glove-wiki-gigaword-100')\n\n# ignore the warning\nwarnings.filterwarnings(\"ignore\")\n\n# Some Kaggle-wordnet patch\n! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet \n\n# Install java for pyserini\n\n!wget \"https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\"\n!tar -xvf openjdk-11.0.2_linux-x64_bin.tar.gz\n\n!export JAVA_HOME='/kaggle/working/jdk-11.0.2/'\n!export PATH='/kaggle/working/jdk-11.0.2/bin':$PATH\n!mkdir -p /kaggle/working/jdk-11.0.2/jre/lib/amd64/server/\n\n!ln -s /kaggle/working/jdk-11.0.2/lib/server/libjvm.so /kaggle/working/jdk-11.0.2/jre/lib/amd64/server/libjvm.so\nos.environ[\"JAVA_HOME\"] = \"/kaggle/working/jdk-11.0.2/\"","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:10:31.380508Z","iopub.execute_input":"2023-04-04T16:10:31.381032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Reading dataset and pre-processing","metadata":{}},{"cell_type":"markdown","source":"#### 1.1 Read dataset","metadata":{}},{"cell_type":"code","source":"# load the dataset\nf_metadata_path = \"../input/CORD-19-research-challenge/metadata.csv\"\n\n# construct the data frame of metadata\ndf = pd.read_csv(f_metadata_path)\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2 Dropping useless columns","metadata":{}},{"cell_type":"code","source":"# drop useless columns for this assignment\ndf_trimed = df.drop([\"sha\", \"source_x\", \"doi\", \"license\", \"authors\", \"journal\", \"pmc_json_files\",\n                    \"pmcid\", \"pubmed_id\", \"mag_id\", \"who_covidence_id\", \"arxiv_id\", \"url\", \"s2_id\"], axis=1)\ndf_trimed.head(5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3 Sampling","metadata":{}},{"cell_type":"markdown","source":"Given that COVID-19 was first reported in December 2019, we can limit our data selection to articles published after this date. Therefore, we will sample data from December 1st, 2019, onwards.","metadata":{}},{"cell_type":"code","source":"# only keep the documents published after 2019-11\ndf_trimed = df_trimed[df_trimed[\"publish_time\"] > '2019-12']\ndf_trimed.shape[0]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the missing values in the dataset.","metadata":{}},{"cell_type":"code","source":"# check the number of missing value in each column\nmissing_values_count = df_trimed.isnull().sum()\nmissing_values_count\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Many rows of data are missing titles, abstracts, and PDF files, as demonstrated above. We will drop these rows from our analysis.","metadata":{}},{"cell_type":"code","source":"df_trimed = df_trimed.dropna(subset=['pdf_json_files', 'title', 'abstract'])\ndf_trimed.shape[0]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will verify the validity of the PDF file URLs. Any rows with invalid URLs will be dropped from the dataset.","metadata":{}},{"cell_type":"code","source":"# remove all the rows that do not have the real file\nimport os\n# tqdm progress apply\ntqdm.pandas(desc=\"removing all the rows that do not have the real file...\")\nprint(df_trimed['pdf_json_files'])\n\n\ndf_trimed = df_trimed[df_trimed['pdf_json_files'].progress_apply(\n    lambda x: os.path.isfile(\"../input/CORD-19-research-challenge/\"+x))]\ndf_trimed.shape[0]\ns","metadata":{"execution":{"iopub.status.idle":"2023-04-04T16:24:16.999339Z","shell.execute_reply.started":"2023-04-04T16:12:37.467049Z","shell.execute_reply":"2023-04-04T16:24:16.998138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the keywords of the test queries, we can sample the data further. We will only keep the rows whose title or abstract contain the keywords in the test queries. ","metadata":{}},{"cell_type":"code","source":"# function: select the rows whose title or abstract contain the given strings, ignoring the case\ndef select_rows_contain_string(df, strings):\n    tqdm.pandas(\n        desc=\"selecting the rows whose title or abstract contain the given strings...\")\n    return df[df.progress_apply(lambda row: any(string in row['title'].lower() or string in row['abstract'].lower() for string in strings), axis=1)]","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:24:17.000704Z","iopub.execute_input":"2023-04-04T16:24:17.001113Z","iopub.status.idle":"2023-04-04T16:24:17.007568Z","shell.execute_reply.started":"2023-04-04T16:24:17.001077Z","shell.execute_reply":"2023-04-04T16:24:17.006672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# title or abstract contain \"COVID-19\", \"SARS-CoV-2\", \"coronavirus\", \"2019-nCoV\", \"covid\", \"covid-19\", \"Covid-19\"\nstrings = [\"COVID-19\", \"SARS-CoV-2\", \"coronavirus\",\n           \"2019-nCoV\", \"covid\", \"covid-19\", \"Covid-19\"]\ndf_trimed_covid = select_rows_contain_string(df_trimed, strings)\n\nkeyword_sampled_df_list = []\n\n# title or abstract contain \"origin\", \"Wuhan\" from the df_trimed_covid\nstrings = [\"origin\", \"Wuhan\"]\ndf_trimed_origin = select_rows_contain_string(df_trimed_covid, strings)\nkeyword_sampled_df_list.append(df_trimed_origin)\n\n# title or abstract contain \"rapid testing\"\nstrings = [\"rapid testing\"]\ndf_trimed_testing = select_rows_contain_string(df_trimed_covid, strings)\nkeyword_sampled_df_list.append(df_trimed_testing)\n\n# title or abstract contain \"social\", \"distancing\", \"lockdown\", \"quarantine\"\nstrings = [\"social distancing\", \"lockdown\", \"quarantine\"]\ndf_trimed_social = select_rows_contain_string(df_trimed_covid, strings)\nkeyword_sampled_df_list.append(df_trimed_social)\n\n# title or abstract contain \"transmission route\"\nstrings = [\"transmission route\"]\ndf_trimed_transmission = select_rows_contain_string(df_trimed_covid, strings)\nkeyword_sampled_df_list.append(df_trimed_transmission)\n\n# title or abstract contain \"best masks\", \"preventing infection\", \"prevent infection\", \"preventing transmission\", \"prevent transmission\", \"preventing spread\", \"prevent spread\"\nstrings = [\"best masks\", \"preventing infection\", \"prevent infection\",\n           \"preventing transmission\", \"prevent transmission\", \"preventing spread\", \"prevent spread\"]\ndf_trimed_masks = select_rows_contain_string(df_trimed_covid, strings)\nkeyword_sampled_df_list.append(df_trimed_masks)\n\n# title or abstract contain \"hand sanitizer\"\nstrings = [\"hand sanitizer\"]\ndf_trimed_sanitizer = select_rows_contain_string(df_trimed_covid, strings)\nkeyword_sampled_df_list.append(df_trimed_sanitizer)\n\n# title or abstract contain \"vaccine\", \"vaccination\", \"vaccines\", \"vaccinations\"\nstrings = [\"vaccine\", \"vaccination\", \"vaccines\", \"vaccinations\"]\ndf_trimed_vaccine = select_rows_contain_string(df_trimed_covid, strings)\nkeyword_sampled_df_list.append(df_trimed_vaccine)\n\n# title or abstract contain \"Vitamin\"\nstrings = [\"Vitamin\"]\ndf_trimed_vitamin = select_rows_contain_string(df_trimed_covid, strings)\nkeyword_sampled_df_list.append(df_trimed_vitamin)\n\n# title or abstract contain \"live outside the body\"\nstrings = [\"live outside the body\"]\ndf_trimed_outside = select_rows_contain_string(df_trimed_covid, strings)\nkeyword_sampled_df_list.append(df_trimed_outside)\n\n# title or abstract contain \"initial symptoms\"\nstrings = [\"initial symptoms\"]\ndf_trimed_symptoms = select_rows_contain_string(df_trimed_covid, strings)\nkeyword_sampled_df_list.append(df_trimed_symptoms)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:24:17.008545Z","iopub.execute_input":"2023-04-04T16:24:17.008917Z","iopub.status.idle":"2023-04-04T16:25:41.000082Z","shell.execute_reply.started":"2023-04-04T16:24:17.008882Z","shell.execute_reply":"2023-04-04T16:25:40.997803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# join all the dataframes above into one dataframe and remove the duplicates\ndf_trimed_covid = pd.concat(keyword_sampled_df_list).drop_duplicates()\ndf_trimed_covid.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:25:41.001324Z","iopub.execute_input":"2023-04-04T16:25:41.001633Z","iopub.status.idle":"2023-04-04T16:25:41.389600Z","shell.execute_reply.started":"2023-04-04T16:25:41.001603Z","shell.execute_reply":"2023-04-04T16:25:41.388634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we randomly sample 10000 rows from the dataset.","metadata":{}},{"cell_type":"code","source":"# randomly pick 10000 documents from the dataset\n\ndf_sampled = df_trimed_covid.sample(n=10000, random_state=42)\ndf_sampled","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:25:41.390941Z","iopub.execute_input":"2023-04-04T16:25:41.391622Z","iopub.status.idle":"2023-04-04T16:25:41.413938Z","shell.execute_reply.started":"2023-04-04T16:25:41.391583Z","shell.execute_reply":"2023-04-04T16:25:41.412666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.4 Pre-processing the text data ","metadata":{}},{"cell_type":"markdown","source":"After sampling the data, our next step is to pre-process the text data. This involves extracting the text from the PDF JSON files as our first task.","metadata":{}},{"cell_type":"code","source":"# function: get the pdf json text based on the URL\ndef extract_pdf(row, pdf_json):\n    # paragraphs other than abstract, introduction, conclusion\n    row[\"other_paragraph\"] = []\n    for body_paragraph in pdf_json['body_text']:\n        if body_paragraph[\"section\"] == \"Introduction\":\n            row[\"introduction\"] = body_paragraph[\"text\"]\n        elif body_paragraph[\"section\"] == \"Conclusion\":\n            row[\"conclusion\"] = body_paragraph[\"text\"]\n        else:\n            row[\"other_paragraph\"].append(body_paragraph[\"text\"])\n    return row\n\n# extract the pdf json text from the sampled dataset\ntqdm.pandas(desc=\"extracting the pdf json text from the sampled dataset...\")\ndf_sampled = df_sampled.progress_apply(lambda row: extract_pdf(\n    row, json.load(open(\"../input/CORD-19-research-challenge/\"+row['pdf_json_files']))), axis=1)\ndf_sampled.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:25:41.415134Z","iopub.execute_input":"2023-04-04T16:25:41.415434Z","iopub.status.idle":"2023-04-04T16:27:25.678188Z","shell.execute_reply.started":"2023-04-04T16:25:41.415405Z","shell.execute_reply":"2023-04-04T16:27:25.676845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we can preprocess the text we got from the PDF JSON files. Preprocessing the text involves several tasks: first, we will convert all text to lowercase and remove stopwords. Next, we will perform lemmatization and store the preprocessed text in a separate column for further analysis.","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\n# function that preprocesses the input string\ndef preprocess_text(string):\n    string = string.lower()\n    # remove stopwords\n    string = \" \".join([word for word in string.split()\n                      if word not in stop_words])\n    # lemmatization\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    string = \" \".join([lemmatizer.lemmatize(word) for word in string.split()])\n    return string\n\n# function that preprocesses the text of a row in the sampled dataset\ndef preprocess_text_in_df(row):\n    title = preprocess_text(row['title'])\n    abstract = preprocess_text(row['abstract'])\n    introduction = \"\"\n    if pd.isnull(row['introduction']) == False:\n        introduction = preprocess_text(row['introduction'])\n    conclusion = \"\"\n    if pd.isnull(row['conclusion']) == False:\n        conclusion = preprocess_text(row['conclusion'])\n    other_paragraph = \"\"\n    for paragraph in row['other_paragraph']:\n        other_paragraph += preprocess_text(paragraph)\n    row[\"preprocessed_text\"] = title + \" \" + abstract + \" \" + \\\n        introduction + \" \" + conclusion + \" \" + other_paragraph\n    return row\n\n\ntqdm.pandas(desc=\"preprocessing the text in the sampled dataset...\")\ndf_sampled = df_sampled.progress_apply(\n    lambda row: preprocess_text_in_df(row), axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:27:25.680052Z","iopub.execute_input":"2023-04-04T16:27:25.680552Z","iopub.status.idle":"2023-04-04T16:29:51.868626Z","shell.execute_reply.started":"2023-04-04T16:27:25.680510Z","shell.execute_reply":"2023-04-04T16:29:51.866751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sampled[\"preprocessed_text\"].head()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:29:51.873864Z","iopub.execute_input":"2023-04-04T16:29:51.874312Z","iopub.status.idle":"2023-04-04T16:29:51.888308Z","shell.execute_reply.started":"2023-04-04T16:29:51.874276Z","shell.execute_reply":"2023-04-04T16:29:51.887026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generating the paragraph list","metadata":{}},{"cell_type":"code","source":"# generate the list of paragraphs\nparagraphs = []\nfor index, row in df_sampled.iterrows():\n    paragraph_obj = {}\n    paragraph_obj['text'] = row['abstract']\n    paragraph_obj['p_id'] = row['cord_uid'] + \"_0\"\n    paragraphs.append(paragraph_obj)\n    # extract the paragraphs from the json file\n    with open(\"./archive/\"+row['pdf_json_files']) as f:\n        json_data = json.load(f)\n        p_index = 1\n        for body in json_data['body_text']:\n            paragraph_obj = {}\n            paragraph_obj['text'] = body['text']\n            paragraph_obj['p_id'] = row['cord_uid'] + \"_\" + str(p_index)\n            paragraphs.append(paragraph_obj)\n            p_index += 1\n\n# turn the list of paragraphs into a dataframe\ndf_paragraphs = pd.DataFrame(paragraphs)\n# set the index of the dataframe to be the p_id\ndf_paragraphs = df_paragraphs.set_index('p_id')\ndf_paragraphs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Named Entity Recognition and Knowledge Base","metadata":{}},{"cell_type":"markdown","source":"#### 2.1 Entity extraction","metadata":{}},{"cell_type":"markdown","source":"To save time, we will only extract entities from the title, abstract, introduction, and conclusion sections of the text. These sections of the article are most likely to contain entities that are relevant to the topic of the article.","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\n\n# function: get the name entities from the title, abstract, introduction, and conclusion of each data in the dataset\ndef get_name_entity(row):\n    name_entity = []\n    doc = nlp(row[\"title\"])\n    for ent in doc.ents:\n        # if ent is not number\n        if ent.label_ != \"CARDINAL\" and ent.label_ != \"PERCENT\" and ent.label_ != \"MONEY\":\n            name_entity.append(ent.text)\n    doc = nlp(row[\"abstract\"])\n    for ent in doc.ents:\n        if ent.label_ != \"CARDINAL\" and ent.label_ != \"PERCENT\" and ent.label_ != \"MONEY\":\n            name_entity.append(ent.text)\n    if pd.isna(row[\"introduction\"]) == False:\n        doc = nlp(row[\"introduction\"])\n        for ent in doc.ents:\n            if ent.label_ != \"CARDINAL\" and ent.label_ != \"PERCENT\" and ent.label_ != \"MONEY\":\n                name_entity.append(ent.text)\n    if pd.isna(row[\"conclusion\"]) == False:\n        doc = nlp(row[\"conclusion\"])\n        for ent in doc.ents:\n            if ent.label_ != \"CARDINAL\" and ent.label_ != \"PERCENT\" and ent.label_ != \"MONEY\":\n                name_entity.append(ent.text)\n    return name_entity\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:29:51.889935Z","iopub.execute_input":"2023-04-04T16:29:51.890256Z","iopub.status.idle":"2023-04-04T16:29:52.752505Z","shell.execute_reply.started":"2023-04-04T16:29:51.890224Z","shell.execute_reply":"2023-04-04T16:29:52.751222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the name entity of the title, abstract, introduction, and conclusion of each data in the dataset\ntqdm.pandas(desc=\"Getting name entity...\")\ndf_sampled['name_entity'] = df_sampled.progress_apply(\n    lambda row: get_name_entity(row), axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:29:52.754086Z","iopub.execute_input":"2023-04-04T16:29:52.754415Z","iopub.status.idle":"2023-04-04T16:42:17.862874Z","shell.execute_reply.started":"2023-04-04T16:29:52.754385Z","shell.execute_reply":"2023-04-04T16:42:17.861768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Presenting the frequency of the extracted entities.","metadata":{}},{"cell_type":"code","source":"# merge the name entity into one list\nname_entity_list = []\nfor name_entity in df_sampled['name_entity']:\n    name_entity_list.extend(name_entity)\n\n# count the frequency of each name entity and sort it\nname_entity_count = Counter(name_entity_list)\nname_entity_count = sorted(name_entity_count.items(),\n                           key=lambda x: x[1], reverse=True)\n\n# show the top 100 name entity\nname_entity_count[:100]\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:42:17.864301Z","iopub.execute_input":"2023-04-04T16:42:17.864817Z","iopub.status.idle":"2023-04-04T16:42:17.947003Z","shell.execute_reply.started":"2023-04-04T16:42:17.864778Z","shell.execute_reply":"2023-04-04T16:42:17.945571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2 Knowledge base","metadata":{}},{"cell_type":"markdown","source":"We will manually build knowledge bases that display synonyms of the entities and their associated keywords. This will be based on the results of the Named Entity Recognition and the test query set.","metadata":{}},{"cell_type":"code","source":"# manually create the knowledge base\n\n# create the knowledge base\nknowledge_base_synonym = {\n    \"COVID-19\": [\"SARS-CoV-2\", \"coronavirus disease 2019\", \"coronavirus disease 19\", \"coronavirus 2019\", \"coronavirus 19\", \"2019 novel coronavirus\", \"2019-nCoV\", \"2019-novel coronavirus\", \"2019 novel coronavirus pneumonia\", \"2019-nCoV pneumonia\"],\n    \"rapid testing\": [\"RAT\", \"rapid test\", \"rapid antigen test\", \"rapid antigen tests\", \"rapid antigen testing\", \"rapid antigen tests\", \"rapid antigen test kit\"],\n    \"origin\": [\"origins\", \"source\", \"sources\"],\n    \"initial symtoms\": [\"early signs\"]\n}\n\nknowledge_base_association = {\n    \"mask\": [\"n95\", \"cloth mask\"],\n    \"vaccine\": [\"mrna\"],\n    \"origin\": [\"wuhan\", \"fish market\"],\n    \"symptoms\": [\"fever\", \"chill\", \"cough\", \"tired\", \"headache\", \"loss taste or small\", \"sore throat\", \"diarrhea\"],\n    \"sanitizer\": [\"alcohol\"],\n    \"social distancing\": [\"quarantine\", \"lockdown\"],\n    \"transmission route\": [\"airborne\", \"droplet\", \"contact\", \"fomite\"],\n    \"testing\": [\"PCR\"]\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:42:17.948303Z","iopub.execute_input":"2023-04-04T16:42:17.948632Z","iopub.status.idle":"2023-04-04T16:42:17.958061Z","shell.execute_reply.started":"2023-04-04T16:42:17.948601Z","shell.execute_reply":"2023-04-04T16:42:17.956794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Indexing method","metadata":{}},{"cell_type":"markdown","source":"To efficiently retrieve documents containing words from the query, we will utilize the inverted index method. This method creates a dictionary that maps words to the documents that contain them.","metadata":{}},{"cell_type":"code","source":"# the unique word set of the preprocessed_text of the dataset\nunique_word_set = set([])\n\ndef get_unique_words(row):\n    for word in row['preprocessed_text'].split():\n        # if not a single puctuation\n        if re.match(r'^[^\\w\\s]$', word) == None:\n            unique_word_set.add(word)\n\n\ntqdm.pandas(desc=\"Getting unique words...\")\nresult = df_sampled.progress_apply(lambda row: get_unique_words(row), axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:42:17.959978Z","iopub.execute_input":"2023-04-04T16:42:17.960464Z","iopub.status.idle":"2023-04-04T16:42:52.545118Z","shell.execute_reply.started":"2023-04-04T16:42:17.960411Z","shell.execute_reply":"2023-04-04T16:42:52.543765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the inverted index of the preprocessed_text of the dataset\ninverted_index = inv_indx = {i: [] for i in unique_word_set}\n\n# function: get the inverted index of the preprocessed_text of the dataset\ndef get_inverted_index(row, inverted_index):\n    for word in row['preprocessed_text'].split():\n        if word in inverted_index:\n            inverted_index[word].append(row['cord_uid'])\n\n\ntqdm.pandas(desc=\"Getting inverted index...\")\nresult = df_sampled.progress_apply(\n    lambda row: get_inverted_index(row, inverted_index), axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:42:52.546514Z","iopub.execute_input":"2023-04-04T16:42:52.546919Z","iopub.status.idle":"2023-04-04T16:44:53.366553Z","shell.execute_reply.started":"2023-04-04T16:42:52.546883Z","shell.execute_reply":"2023-04-04T16:44:53.365209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the documents containing the word \"mask\"\ninverted_index[\"mask\"]\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:44:53.368446Z","iopub.execute_input":"2023-04-04T16:44:53.369272Z","iopub.status.idle":"2023-04-04T16:44:53.394096Z","shell.execute_reply.started":"2023-04-04T16:44:53.369221Z","shell.execute_reply":"2023-04-04T16:44:53.392915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Text matching utility","metadata":{}},{"cell_type":"markdown","source":"To obtain the answer snippets to a query, we will first extend the query by adding related entities from the knowledge base. Next, we will use the inverted index method to locate documents containing the words from the extended query. We will then calculate cosine similarity between the query and the documents to rank them and select the top 5 as the target documents. Finally, we will again use cosine similarity to rank the paragraphs in the target documents and extract the top 3 paragraphs as answer snippets.","metadata":{}},{"cell_type":"markdown","source":"First, we define the function to preprocess the query.","metadata":{}},{"cell_type":"code","source":"# function: extend the query with the knowledge base\ndef extend_query(query, knowledge_base_synonym, knowledge_base_association):\n    # convert the query to lower case\n    query = query.lower()\n    # for each key in the knowledge base, if the key is in the query, then add the value to the query string\n    for key in knowledge_base_synonym:\n        # if the query string contains the key\n        if key.lower() in query:\n            # concatenate the query string with each value of the key\n            for value in knowledge_base_synonym[key]:\n                query += \" \" + value\n    for key in knowledge_base_association:\n        if key.lower() in query:\n            for value in knowledge_base_association[key]:\n                query += \" \" + value\n    return query\n\n# query preprocessing: extend the query with the knowledge base, remove stopwords, lemmatization, and remove question mark\ndef preprocess_query(query):\n    # extend the query with the knowledge base\n    query = extend_query(query, knowledge_base_synonym,\n                         knowledge_base_association)\n    query = query.lower()\n    # remove stopwords\n    query = \" \".join([word for word in query.split()\n                     if word not in stop_words])\n    # lemmatization\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    query = \" \".join([lemmatizer.lemmatize(word) for word in query.split()])\n    # remove question mark\n    query = query.replace(\"?\", \"\")\n    return query\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:44:53.395867Z","iopub.execute_input":"2023-04-04T16:44:53.396430Z","iopub.status.idle":"2023-04-04T16:44:53.410593Z","shell.execute_reply.started":"2023-04-04T16:44:53.396383Z","shell.execute_reply":"2023-04-04T16:44:53.409636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we need to define the function to embed the query and the documents.","metadata":{}},{"cell_type":"code","source":"# function: get the vector representation of the query by averaging the vector representation of each word in the query\ndef embedding_string(string):\n    vectors = []\n    for word in string.split():\n        if word in glove_model:\n            vectors.append(glove_model[word])\n    if vectors == []:\n        return np.zeros(100)\n    return np.mean(vectors, axis=0)\n\n# get the vector representation of each document in df_sampled\ndf_sampled['embedding'] = df_sampled.progress_apply(\n    lambda row: embedding_string(row['preprocessed_text']), axis=1)\n\ndf_sampled['embedding']","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:44:53.411886Z","iopub.execute_input":"2023-04-04T16:44:53.412416Z","iopub.status.idle":"2023-04-04T16:46:15.426129Z","shell.execute_reply.started":"2023-04-04T16:44:53.412371Z","shell.execute_reply":"2023-04-04T16:46:15.424714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.1 Cosine similarity","metadata":{}},{"cell_type":"code","source":"# function: calculate the cosine similarity between two vectors\ndef cosine_similarity(vector1, vector2):\n    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:46:15.427439Z","iopub.execute_input":"2023-04-04T16:46:15.427766Z","iopub.status.idle":"2023-04-04T16:46:15.432612Z","shell.execute_reply.started":"2023-04-04T16:46:15.427716Z","shell.execute_reply":"2023-04-04T16:46:15.431664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2 Get the top n documents or paragraphs","metadata":{}},{"cell_type":"code","source":"# function: get the top n documents that are most similar to the query from the indexed document set\ndef get_top_n_documents(query_vector, indexed_document_set, n):\n    # get the vector representation of each document in the indexed document set\n    df_indexed = df_sampled[df_sampled['cord_uid'].isin(indexed_document_set)]\n    # calculate the cosine similarity between the query and each document in the indexed document set\n    df_indexed['similarity'] = df_indexed.progress_apply(\n        lambda row: cosine_similarity(query_vector, row['embedding']), axis=1)\n    # sort the documents by the similarity score\n    df_indexed = df_indexed.sort_values(by=['similarity'], ascending=False)\n    # get the top n documents\n    df_top_n = df_indexed[:n]\n    return df_top_n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:46:15.433555Z","iopub.execute_input":"2023-04-04T16:46:15.433929Z","iopub.status.idle":"2023-04-04T16:46:15.444211Z","shell.execute_reply.started":"2023-04-04T16:46:15.433891Z","shell.execute_reply":"2023-04-04T16:46:15.442817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function: get the top n paragraphs that are most similar to the query for each document in the top n documents\ndef get_top_n_paragraphs(query_vector, df_top_n_doc, n):\n    top_n_paragraphs_dict = {}\n    for index, row in df_top_n_doc.iterrows():\n        # get the paragraphs of the article\n        paragraph_list = {\"paragraph\": [], \"similarity\": []}\n        paragraph_list[\"paragraph\"].append(row[\"abstract\"])\n\n        # if the conclusion is not null, then add it to the paragraph list\n        if pd.isnull(row[\"conclusion\"]) == False:\n            paragraph_list[\"paragraph\"].append(row[\"conclusion\"])\n\n        if pd.isnull(row[\"introduction\"]) == False:\n            paragraph_list[\"paragraph\"].append(row[\"introduction\"])\n\n        paragraph_list[\"paragraph\"].extend(row[\"other_paragraph\"])\n\n        # calculate the cosine similarity between the query and each paragraph\n        for paragraph in paragraph_list[\"paragraph\"]:\n            paragraph_list[\"similarity\"].append(cosine_similarity(\n                query_vector, embedding_string(paragraph)))\n        \n        df_paragraph_list = pd.DataFrame(paragraph_list)\n        # sort the paragraphs by the similarity score\n        df_paragraph_list = df_paragraph_list.sort_values(\n            by=['similarity'], ascending=False)\n        top_n_paragraphs_dict[row[\"cord_uid\"]] = df_paragraph_list[:n]\n    return top_n_paragraphs_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:46:15.445823Z","iopub.execute_input":"2023-04-04T16:46:15.446211Z","iopub.status.idle":"2023-04-04T16:46:15.456513Z","shell.execute_reply.started":"2023-04-04T16:46:15.446177Z","shell.execute_reply":"2023-04-04T16:46:15.455371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.3 Get the final answer of a query","metadata":{}},{"cell_type":"code","source":"# function: integrate the above functions to get the answer of the query\ndef get_answer(query, n):\n    # preprocess the query\n    query = preprocess_query(query)\n    # get the indexed document set\n    indexed_document_set = set([])\n    for word in query.split():\n        if word in inverted_index:\n            for document in inverted_index[word]:\n                indexed_document_set.add(document)\n    # get the vector representation of the query\n    query_vector = embedding_string(query)\n    # get the top n documents that are most similar to the query\n    df_top_n = get_top_n_documents(query_vector, indexed_document_set, n)\n    # get the top n paragraphs that are most similar to the query for each document in the top n documents\n    top_n_paragraphs_dict = get_top_n_paragraphs(query_vector, df_top_n, 3)\n    return top_n_paragraphs_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:46:15.460912Z","iopub.execute_input":"2023-04-04T16:46:15.461317Z","iopub.status.idle":"2023-04-04T16:46:15.470155Z","shell.execute_reply.started":"2023-04-04T16:46:15.461253Z","shell.execute_reply":"2023-04-04T16:46:15.469177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Test utility and test results","metadata":{}},{"cell_type":"code","source":"# the test set with 10 queries\ntest_set = [\n    \"what is the origin of COVID-19?\",\n    \"what types of rapid testing for Covid-19 have been developed?\",\n    \"has social distancing had an impact on slowing the spread of COVID-19?\",\n    \"what are the transmission routes of coronavirus?\",\n    \"what are the best masks for preventing infection by Covid-19?\",\n    \"what type of hand sanitizer is needed to destroy Covid-19?\",\n    \"What vaccine candidates are being tested for Covid-19?\",\n    \"does Vitamin D impact COVID-19 prevention and treatment?\",\n    \"how long can the coronavirus live outside the body?\",\n    \"what are the initial symptoms of Covid-19?\",\n]\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:46:15.471475Z","iopub.execute_input":"2023-04-04T16:46:15.472217Z","iopub.status.idle":"2023-04-04T16:46:15.484471Z","shell.execute_reply.started":"2023-04-04T16:46:15.472182Z","shell.execute_reply":"2023-04-04T16:46:15.483184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# present the top_n_paragraphs_dict in a more readable way\ndef present_answer(top_n_paragraphs_dict):\n    for key in top_n_paragraphs_dict:\n        # print the document title\n        print(\"Document: \" +\n              df_sampled[df_sampled[\"cord_uid\"] == key][\"title\"].values[0])\n        for index, row in top_n_paragraphs_dict[key].iterrows():\n            print(\"Paragraph: \" + row[\"paragraph\"])\n            print(\"Confidence: \" + str(row[\"similarity\"]))\n            print(\"\")\n        print(\"---------------------------------------------------------------------------------------------\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:46:15.485944Z","iopub.execute_input":"2023-04-04T16:46:15.487338Z","iopub.status.idle":"2023-04-04T16:46:15.494838Z","shell.execute_reply.started":"2023-04-04T16:46:15.487297Z","shell.execute_reply":"2023-04-04T16:46:15.493788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(test_set)):\n    print(\"Question \" + str(i + 1) + \": \" + test_set[i])\n    present_answer(get_answer(test_set[i], 5))\n    print(\"\")\n    print(\"=================================================================================================\")\n    print(\"=================================================================================================\")\n    print(\"\")\n    print(\"\")\n","metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-04-04T16:46:15.498231Z","iopub.execute_input":"2023-04-04T16:46:15.499020Z","iopub.status.idle":"2023-04-04T16:46:19.717973Z","shell.execute_reply.started":"2023-04-04T16:46:15.498962Z","shell.execute_reply":"2023-04-04T16:46:19.716781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.1 Calculate the MRR of the result","metadata":{}},{"cell_type":"markdown","source":"After obtaining the test results, we will manually identify the correctness of each return snippets and calculate the reciprocal rank for each query.","metadata":{}},{"cell_type":"code","source":"reciprocal_rank = np.zeros(len(test_set))\nreciprocal_rank[0] = 1/2 # the second document is a relevant document\nreciprocal_rank[1] = 1/5 # the fifth document is a relevant document\nreciprocal_rank[2] = 0 # there is no relevant document in the top 5 documents\nreciprocal_rank[3] = 1 # the first document is a relevant document\nreciprocal_rank[4] = 0\nreciprocal_rank[5] = 0\nreciprocal_rank[6] = 1/4\nreciprocal_rank[7] = 0\nreciprocal_rank[8] = 0\nreciprocal_rank[9] = 1\n\nprint(\"The mean reciprocal rank of the test set is: \" +\n      str(np.mean(reciprocal_rank)))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:46:19.719590Z","iopub.execute_input":"2023-04-04T16:46:19.719930Z","iopub.status.idle":"2023-04-04T16:46:19.727890Z","shell.execute_reply.started":"2023-04-04T16:46:19.719897Z","shell.execute_reply":"2023-04-04T16:46:19.726683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.2 Result analysis","metadata":{}},{"cell_type":"markdown","source":"The result shows that only half of the queries are accurately answered, which are: 1. \"what is the origin of COVID-19?\"; 2. \"what types of rapid testing for Covid-19 have been developed?\"; 3. \"what are the transmission routes of coronavirus?\"; 4. \"What vaccine candidates are being tested for Covid-19?\"; 5. \"what are the initial symptoms of Covid-19?\". The success of these queries can be attributed to the presence of specific keywords in the knowledge base. Conversely, queries that were answered incorrectly lacked keywords that were not present in the knowledge base or dataset. Therefore, it can be concluded that a well-constructed knowledge base is critical for precise answer retrieval, which can be enhanced by incorporating additional keywords and synonyms or by utilizing relation extraction to establish the connections between entities. Despite these limitations, the outcomes highlight the potential of utilizing named entity recognition and knowledge bases for question-answering systems.","metadata":{}},{"cell_type":"markdown","source":"### 6. Simple user interface","metadata":{}},{"cell_type":"code","source":"convert_json_list = []\ndef convert_to_json(row):\n    num = 0\n    row['other_paragraph'].append(row['introduction'])\n     row['other_paragraph'].another_paragraph']:\n        convert_json_list.append(\n        {\n            \"id\": row[\"cord_uid\"]+\"-\"+ str(num),\n            \"contents\": i\n        })\n        num = num+1;\n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:47:19.563351Z","iopub.execute_input":"2023-04-04T16:47:19.563776Z","iopub.status.idle":"2023-04-04T16:47:19.569996Z","shell.execute_reply.started":"2023-04-04T16:47:19.563738Z","shell.execute_reply":"2023-04-04T16:47:19.568776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = df_sampled.progress_apply(\n    lambda row: convert_to_json(row), axis=1)\n\njson_str = json.dumps(convert_json_list)\nwith open(\"collection.json\", \"w\") as outfile:\n    outfile.write(json_str)\nprint(json_str[0:500])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:47:24.342393Z","iopub.execute_input":"2023-04-04T16:47:24.342856Z","iopub.status.idle":"2023-04-04T16:47:28.430420Z","shell.execute_reply.started":"2023-04-04T16:47:24.342816Z","shell.execute_reply":"2023-04-04T16:47:28.429108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m pyserini.index.lucene \\\n  --collection JsonCollection \\\n  --input ./ \\\n  --index index \\\n  --generator DefaultLuceneDocumentGenerator \\\n  --threads 1 \\\n  --storePositions --storeDocvectors --storeRaw","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:49:33.453211Z","iopub.execute_input":"2023-04-04T16:49:33.453634Z","iopub.status.idle":"2023-04-04T16:50:31.932942Z","shell.execute_reply.started":"2023-04-04T16:49:33.453583Z","shell.execute_reply":"2023-04-04T16:50:31.931534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyserini.search.lucene import LuceneSearcher\nimport json\nsearcher = LuceneSearcher('./index')\nf = open(\"collection.json\", \"r\")\ntext = json.loads(f.read())\n\n\nhits = searcher.search('where did coronavirus origin')\n\nfor i in range(len(hits)):\n   \n    print(f'{i+1:2} {hits[i].docid} {hits[i].contents} {hits[i].score:.5f}')\n    for j in text:\n        \n        if(j['id'] == hits[i].docid):\n            print(j['contents'])","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:54:02.431260Z","iopub.execute_input":"2023-04-04T16:54:02.432342Z","iopub.status.idle":"2023-04-04T16:54:09.623339Z","shell.execute_reply.started":"2023-04-04T16:54:02.432291Z","shell.execute_reply":"2023-04-04T16:54:09.622127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accept the query from the user\nquery = input(\"Please enter your question: \")\nprint(query)\n# get the answer of the query\ntop_n_paragraphs_dict = get_answer(query, 5)\n# present the answer\npresent_answer(top_n_paragraphs_dict)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T16:46:19.729881Z","iopub.execute_input":"2023-04-04T16:46:19.730217Z","iopub.status.idle":"2023-04-04T16:47:12.309351Z","shell.execute_reply.started":"2023-04-04T16:46:19.730186Z","shell.execute_reply":"2023-04-04T16:47:12.307609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bert QA","metadata":{}},{"cell_type":"code","source":"# connect with the summarization section\ntest_summerized_reference = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\ntest_query = \"What does the 'B' in BERT stand for?\"","metadata":{"execution":{"iopub.status.busy":"2023-04-05T06:21:25.676542Z","iopub.execute_input":"2023-04-05T06:21:25.676963Z","iopub.status.idle":"2023-04-05T06:21:25.684095Z","shell.execute_reply.started":"2023-04-05T06:21:25.676927Z","shell.execute_reply":"2023-04-05T06:21:25.682409Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install transformers\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-04-05T06:21:28.156788Z","iopub.execute_input":"2023-04-05T06:21:28.157216Z","iopub.status.idle":"2023-04-05T06:21:40.257644Z","shell.execute_reply.started":"2023-04-05T06:21:28.157182Z","shell.execute_reply":"2023-04-05T06:21:40.256095Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertForQuestionAnswering\n\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')","metadata":{"execution":{"iopub.status.busy":"2023-04-05T06:21:56.701616Z","iopub.execute_input":"2023-04-05T06:21:56.702069Z","iopub.status.idle":"2023-04-05T06:22:00.465348Z","shell.execute_reply.started":"2023-04-05T06:21:56.702024Z","shell.execute_reply":"2023-04-05T06:22:00.464115Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')","metadata":{"execution":{"iopub.status.busy":"2023-04-05T06:23:05.701940Z","iopub.execute_input":"2023-04-05T06:23:05.702340Z","iopub.status.idle":"2023-04-05T06:23:06.062717Z","shell.execute_reply.started":"2023-04-05T06:23:05.702306Z","shell.execute_reply":"2023-04-05T06:23:06.061570Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def answer_question(question, answer_text):\n    '''\n    Takes a `question` string and an `answer_text` string (which contains the\n    answer), and identifies the words within the `answer_text` that are the\n    answer. Prints them out.\n    '''\n    # ======== Tokenize ========\n    # Apply the tokenizer to the input text, treating them as a text-pair.\n    input_ids = tokenizer.encode(question, answer_text)\n\n    # Report how long the input sequence is.\n#     print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n\n    # ======== Set Segment IDs ========\n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n\n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n\n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n\n    # ======== Evaluate ========\n    # Run our example through the model.\n    outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n                    return_dict=True) \n\n    start_scores = outputs.start_logits\n    end_scores = outputs.end_logits\n\n    # ======== Reconstruct Answer ========\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n\n    # Get the string versions of the input tokens.\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Start with the first token.\n    answer = tokens[answer_start]\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n        \n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        \n        # Otherwise, add a space then the token.\n        else:\n            answer += ' ' + tokens[i]\n\n    print('Answer: \"' + answer + '\"')","metadata":{"execution":{"iopub.status.busy":"2023-04-05T06:23:09.758130Z","iopub.execute_input":"2023-04-05T06:23:09.758551Z","iopub.status.idle":"2023-04-05T06:23:09.770242Z","shell.execute_reply.started":"2023-04-05T06:23:09.758515Z","shell.execute_reply":"2023-04-05T06:23:09.769060Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"answer_question(test_query, test_summerized_reference)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T06:23:12.990822Z","iopub.execute_input":"2023-04-05T06:23:12.991999Z","iopub.status.idle":"2023-04-05T06:23:14.760559Z","shell.execute_reply.started":"2023-04-05T06:23:12.991941Z","shell.execute_reply":"2023-04-05T06:23:14.759663Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Answer: \"bidirectional encoder representations from transformers\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 7. References","metadata":{}}]}